---
layout: post
title:  "Perceptron"
date:   2017-01-06 18:32:34 +0530
categories: machine-learning
---
The meaning of the word **perceptron** as per the English dictionary is "a computer model or computerized machine devised to represent or simulate the ability of the brain to recognize and discriminate". This definition holds pretty true even when we talk in the context of Machine Learning. A Perceptron is a _Supervised Classification_ algorithm.

<!--more-->

The algorithm is very true to its dictionary meaning and its origins are rather interesting as well.
Its origins date back to the 1950s and it was one the first ever _Neural Networks_ to be implemented. In a sense, the Perceptron learning algorithm is the fore-father of modern day Neural and Deep Learning Networks.



A Perceptron is a very simple yet elegant algorithm which is proven to work given the data meets its required constraints of linear separability. Invented and developed by Frank Rosenblatt in 1957, Perceptrons showcased the ability to classify images. The algorithm raised may eyebrows and interest alike. Further research and experimentation led to its proof of convergance along with identification of its limitations. See [Minsky and Papert's book](https://en.wikipedia.org/wiki/Perceptrons_(book))

This algorithm carries a sort of special meaning for me personally as well. Perceptron happens to be the very first learning algorithm we discussed and implemented as part of our course Machine Learning 101 at IIIT-Bangalore. The following is a snapshot of my class notes (you may overlook the handwriting please). #nostalgia
![alt text][perceptron_notebook_snapshot]


## The Classic Perceptron

A Perceptron is a Supervised Classification Algorithm. In its most basic and classic sense, it is a binary classifier which is proven to converge and find a classifier for linearly separable dataset.
Before we discuss the actual algorithm, let us build up a story around it. Say we have dataset consisting of two kind objects which are known to be [linearly separable](https://en.wikipedia.org/wiki/Linear_separability). Though it might be difficult to find linearly separable datasets in real life, such is the constraint for a Perceptron to work. Even though this may sound as a very limiting constraint, Perceptrons are very powerful along with the fact that they help us understand other more complex learning algorithms like Neural Networks and so on.  

For the sake of this post, let us assume the following data points have been generated by some unknown function $$f(X)$$. These data points can be visualized in a 2-D space as follows:
![alt text][sample_dataset]

As we can see in the above dataset, there are two different classes, one represented as red and the other as green. Our job here is to train a perceptron which can classify the data points 

## Enhancements

## Resources


[perceptron_notebook_snapshot]: {{site.baseurl}}/public/img//perceptron_notebook_snapshot.jpg "A snapshot from my notebook"
[sample_dataset]: {{site.baseurl}}/public/img//sample_dataset.PNG "A snapshot from my notebook"
